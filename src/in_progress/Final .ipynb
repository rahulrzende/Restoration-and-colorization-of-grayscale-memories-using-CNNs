{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage \n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import skimage \n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2lab\n",
    "import os\n",
    "#---------------------------------------------------------------------------\n",
    "# import TensorFlow and related modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Reshape, Dropout, LeakyReLU, BatchNormalization, Input, Concatenate, Activation, concatenate\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a network for training on our dataset, use the pretrained MobileNet for deep layers\n",
    "# prepare the kernel initializer values\n",
    "weight_init = RandomNormal(stddev=0.02)\n",
    "# prepare the Input layer\n",
    "net_input = Input((224,224,3))\n",
    "# download mobile net, and use it as the base.\n",
    "mobile_net_base = MobileNetV2(include_top=False, input_shape=(224,224,3), weights='imagenet')\n",
    "mobilenet = mobile_net_base(net_input)\n",
    "# encoder block #\n",
    "# 224x224\n",
    "conv1 = Conv2D(64, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(net_input)\n",
    "conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "# 112x112\n",
    "conv2 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(conv1)\n",
    "conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "# 112x112\n",
    "conv3 = Conv2D(128, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv2)\n",
    "conv3 =  Activation('relu')(conv3)\n",
    "# 56x56\n",
    "conv4 = Conv2D(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv3)\n",
    "conv4 = Activation('relu')(conv4)\n",
    "# 28x28\n",
    "conv4_ = Conv2D(256, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(conv4)\n",
    "conv4_ = Activation('relu')(conv4_)\n",
    "# 28x28\n",
    "conv5 = Conv2D(512, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv4_)\n",
    "conv5 = Activation('relu')(conv5)\n",
    "# 14x14\n",
    "conv5_ = Conv2D(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv5)\n",
    "conv5_ = Activation('relu')(conv5_)\n",
    "#7x7\n",
    "# fusion layer - connect MobileNet with our encoder\n",
    "conc = concatenate([mobilenet, conv5_])\n",
    "fusion = Conv2D(512, (1, 1), padding='same', kernel_initializer=weight_init)(conc)\n",
    "fusion = Activation('relu')(fusion)\n",
    "# skip fusion layer\n",
    "skip_fusion = concatenate([fusion, conv5_])\n",
    "# decoder block #\n",
    "# 7x7\n",
    "decoder = Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_fusion)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Dropout(0.25)(decoder)\n",
    "# skip layer from conv5 (with added dropout)\n",
    "skip_4_drop = Dropout(0.25)(conv5)\n",
    "skip_4 = concatenate([decoder, skip_4_drop])\n",
    "# 14x14\n",
    "decoder = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_4)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Dropout(0.25)(decoder)\n",
    "# skip layer from conv4_ (with added dropout)\n",
    "skip_3_drop = Dropout(0.25)(conv4_)\n",
    "skip_3 = concatenate([decoder, skip_3_drop])\n",
    "# 28x28\n",
    "decoder = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_3)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Dropout(0.25)(decoder)\n",
    "# 56x56\n",
    "decoder = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Dropout(0.25)(decoder)\n",
    "# 112x112\n",
    "decoder = Conv2DTranspose(64, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "# 112x112\n",
    "decoder = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "# 224x224\n",
    "# output layer, with 2 channels (a and b)\n",
    "output_layer = Conv2D(2, (1, 1), activation='tanh')(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure model\n",
    "model = Model(net_input, output_layer)\n",
    "model.compile(Adam(lr=0.0002), loss='mse', metrics=['accuracy'])\n",
    "#---------------------------------------------------------------------------\n",
    "# load weights\n",
    "model.load_weights('trained_on_all_is_for_100_es.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model, image_l):\n",
    "    # repeat the L value to match input shape\n",
    "    image_l_R = np.repeat(image_l[..., np.newaxis], 3, -1)\n",
    "    image_l_R = image_l_R.reshape((1, 224, 224, 3))\n",
    "    # normalize the input\n",
    "    image_l_R = (image_l_R.astype('float32') - 127.5) / 127.5\n",
    "    # make prediction\n",
    "    prediction = model.predict(image_l_R)\n",
    "    # normalize the output\n",
    "    pred = (prediction[0].astype('float32') * 127.5) + 127.5\n",
    "    return pred\n",
    "\n",
    "def get_LAB(image_l, image_ab):\n",
    "    image_l = image_l.reshape((224, 224, 1))\n",
    "    image_lab = np.concatenate((image_l, image_ab), axis=2)\n",
    "    image_lab = image_lab.astype(\"uint8\")\n",
    "    image_rgb = cv.cvtColor(image_lab, cv.COLOR_LAB2RGB)\n",
    "    image_rgb = Image.fromarray(image_rgb)\n",
    "    return image_rgb\n",
    "\n",
    "def create_sample(model, image_gray):\n",
    "    # get the model's prediction\n",
    "    pred = get_pred(model, image_gray)\n",
    "    # combine input and output to LAB image\n",
    "    image = get_LAB(image_gray, pred)\n",
    "    # create new combined image, save it\n",
    "    new_image = Image.new('RGB', (224, 224))\n",
    "    new_image.paste(image, (0, 0))\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(Selection,l,i):\n",
    "    \"\"\"Selects the image based on user choice and sends it to the neural model for processing\"\"\"\n",
    "    index = l.index(Selection)\n",
    "    img = i[index]\n",
    "    mod_img = resize(rgb2lab(img.copy())[:,:,0], (224,224), anti_aliasing=True)\n",
    "    sample = create_sample(model, mod_img)\n",
    "    sample.save('output/output.jpg')\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols = 2, figsize = (15,5))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(Selection)\n",
    "\n",
    "    axes[1].imshow(sample)\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('Auto-Colored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_restoration(img):\n",
    "    \"\"\"code to carry out image restoration process\"\"\"\n",
    "    #perform denoising of the image\n",
    "    denoised = cv.fastNlMeansDenoisingColored(img,None,7,10,6,21)\n",
    "    #canny edge detection\n",
    "    edges = cv.Canny(denoised,200,250)\n",
    "    #filter for image processing\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    #image dilation\n",
    "    dilation = cv.morphologyEx(edges, cv.MORPH_DILATE, kernel)\n",
    "    closing = cv.morphologyEx(dilation, cv.MORPH_CLOSE, kernel)\n",
    "    erode = cv.morphologyEx(closing,cv.MORPH_ERODE, kernel)\n",
    "    #fill in missing gaps \n",
    "    inpainted = cv.inpaint(denoised,erode,5,cv.INPAINT_TELEA)\n",
    "    #overlay denoised image over smoothed out image\n",
    "    alpha = 0.5\n",
    "    overlaid = inpainted.copy()\n",
    "    cv.addWeighted(denoised, alpha, overlaid, 1 - alpha,0, overlaid)\n",
    "    \n",
    "    #convert image to gray\n",
    "    img2gray = cv.cvtColor(denoised,cv.COLOR_BGR2GRAY)\n",
    "    #create mask and inverse mask based on threshold\n",
    "    ret, mask = cv.threshold(img2gray, 100, 255, cv.THRESH_BINARY_INV)\n",
    "    #combine via bit addition denoised image human and smoothed out background of inpainted image\n",
    "    bg1 = cv.bitwise_and(denoised,denoised,mask = mask)\n",
    "    mask_inv = cv.bitwise_not(mask)\n",
    "    bg2 = cv.bitwise_and(overlaid,overlaid,mask = mask_inv)\n",
    "    combined = cv.add(bg1,bg2)\n",
    "    \n",
    "    #store the various processed images\n",
    "    images = [img,denoised,inpainted,overlaid,combined]\n",
    "    labels = ['Original','Choice 1','Choice 2', 'Choice 3', 'Choice 4']\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images,labels):\n",
    "    \"\"\"displays the various processed images and associated labels with it\"\"\"\n",
    "    fig, axes = plt.subplots(ncols = 5, figsize = (15,5))\n",
    "    axes[0].imshow(images[0])\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(labels[0])\n",
    "\n",
    "    axes[1].imshow(images[1])\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title(labels[1])\n",
    "    \n",
    "    axes[2].imshow(images[2])\n",
    "    axes[2].axis('off')\n",
    "    axes[2].set_title(labels[2])\n",
    "    \n",
    "    axes[3].imshow(images[3])\n",
    "    axes[3].axis('off')\n",
    "    axes[3].set_title(labels[3])\n",
    "    \n",
    "    axes[4].imshow(images[4])\n",
    "    axes[4].axis('off')\n",
    "    axes[4].set_title(labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb86396ad108472fa2b2229317c87aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Enter the name of the picture you want to restore:'), HBox(children=(Text(value='<â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92afcb7dd1fa4817b7e453f149398cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interact_manual, fixed, Output\n",
    "\n",
    "#create button for widget\n",
    "button = widgets.Button(description=\"Begin Restoration\")\n",
    "save = widgets.Button(description=\"Save Result\")\n",
    "#create text box for widget\n",
    "filename = widgets.Text(value='<filename>.jpg',placeholder='Type something',disabled=False)\n",
    "#create output area for widget\n",
    "output = widgets.Output()\n",
    "\n",
    "#layout setting piece for code\n",
    "vertical = widgets.VBox([widgets.Label(value=\"Enter the name of the picture you want to restore:\"),widgets.HBox([filename, button])])\n",
    "display(vertical)\n",
    "display(output)\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    \"\"\"handles the button clicked event\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        #read the image\n",
    "        img = io.imread(\"images/\"+filename.value)\n",
    "        #gets 4 different kinds of processed images\n",
    "        images, labels = img_restoration(img)\n",
    "        #displays the various options\n",
    "        display_images(images,labels)\n",
    "        display(widgets.Label(value=\"Select the picture you want to color:\"))\n",
    "        #code that calls the model based on user selected image\n",
    "        interact(show_image, Selection = labels, l=fixed(labels), i=fixed(images), description = 'Choose image to color')\n",
    "        display(widgets.Label(value=\"Output saved to the output folder\"))\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
