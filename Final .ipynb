{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The primary motivation for us in coming up with this project was to apply a combination of the image manipulation/restoration techniques taught in this class (using skimage, etc.) with teachings from our other courses taken as a part of the MSIM program that had to do with Data Science.\n",
    "- Having said so, we would like to acknowledge that we are all new to both the fields of image processing and deep learning.\n",
    "- Now that that's established, we would like to explain the problem statement:\n",
    "    - Taking old age grayscale/sepia images that have been creased/faded/etc. over time and applying image restoration techniques like denoising (to get rid of extra-graininess in images) & inpainting (to get rid of creases and fading caused over time).\n",
    "    - Taking these restored images and colorizing them (to sort of simulate how the true colors of clothes, objects, landscape, etc. might have been in those times at the time of capture) using a deep learning model trained on a large set of images available on the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MIRFLICKR25k dataset : \n",
    "- 25,000 Flickr images under the Creative Commons license\n",
    "- Each image is 224 by 224 pixels in size\n",
    "- The images are converted into LAB space, then split into 2 parts - one with the L channel and the other with the AB channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all required libraries\n",
    "\n",
    "#### Summary - \n",
    "\n",
    "- Numpy for stacking images in an array\n",
    "- OpenCV for image restoration\n",
    "- Matplotlib for general plotting tasks\n",
    "- Skimage for image restoration\n",
    "- Pandas for general python data manipulations\n",
    "- Random for selecting random images from dataset for testing colorization feature\n",
    "- PIL for Image manipulation\n",
    "- OS for file manipulation, etc.\n",
    "- TensorFlow for foundational support with creation of deep learning model\n",
    "- Keras for high-level experimentation with deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage \n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import skimage \n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2lab\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Reshape, Dropout, LeakyReLU, BatchNormalization, Input, Concatenate, Activation, concatenate\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define structure for a convolutional encoder-decoder deep learning model that colorizes grayscale images\n",
    "\n",
    "The main inspiration for this approach was this paper published in 2017 - \n",
    "https://arxiv.org/abs/1712.03400\n",
    "\n",
    "#### The below code block does following actions -\n",
    "\n",
    "- Randomly initialize the weights for our neural network (to satisfy the expectation of SGD i.e. Stochastic Gradient Descent)\n",
    "- Initialize an input layer that takes in images of the size 224 by 224 pixels\n",
    "- Use the pretrained (imagenet data) neural network MobileNetV2 for high-level feature extraction\n",
    "- Dropouts are implemented to prevent overfitting (this is done by dropping neurons at certain stages to train different neural networks each time)\n",
    "- The output layer is configured for A and B channels (intended to be merged with the L channel) of our input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a network for training on our dataset, use the pretrained MobileNet for deep layers\n",
    "\n",
    "# prepare the kernel initializer values\n",
    "weight_init = RandomNormal(stddev=0.02)\n",
    "\n",
    "# prepare the Input layer\n",
    "net_input = Input((224,224,3))\n",
    "\n",
    "# download mobile net, and use it as the base.\n",
    "mobile_net_base = MobileNetV2(include_top=False, input_shape=(224,224,3), weights='imagenet')\n",
    "mobilenet = mobile_net_base(net_input)\n",
    "\n",
    "# encoder block #\n",
    "\n",
    "# 224x224\n",
    "conv1 = Conv2D(64, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(net_input)\n",
    "conv1 = LeakyReLU(alpha=0.2)(conv1)\n",
    "\n",
    "# 112x112\n",
    "conv2 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(conv1)\n",
    "conv2 = LeakyReLU(alpha=0.2)(conv2)\n",
    "\n",
    "# 112x112\n",
    "conv3 = Conv2D(128, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv2)\n",
    "conv3 =  Activation('relu')(conv3)\n",
    "\n",
    "# 56x56\n",
    "conv4 = Conv2D(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv3)\n",
    "conv4 = Activation('relu')(conv4)\n",
    "\n",
    "# 28x28\n",
    "conv4_ = Conv2D(256, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(conv4)\n",
    "conv4_ = Activation('relu')(conv4_)\n",
    "\n",
    "# 28x28\n",
    "conv5 = Conv2D(512, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv4_)\n",
    "conv5 = Activation('relu')(conv5)\n",
    "\n",
    "# 14x14\n",
    "conv5_ = Conv2D(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv5)\n",
    "conv5_ = Activation('relu')(conv5_)\n",
    "\n",
    "#7x7\n",
    "# fusion layer - connect MobileNet with our encoder\n",
    "conc = concatenate([mobilenet, conv5_])\n",
    "fusion = Conv2D(512, (1, 1), padding='same', kernel_initializer=weight_init)(conc)\n",
    "fusion = Activation('relu')(fusion)\n",
    "\n",
    "# skip fusion layer\n",
    "skip_fusion = concatenate([fusion, conv5_])\n",
    "\n",
    "# decoder block #\n",
    "\n",
    "# 7x7\n",
    "decoder = Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_fusion)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Dropout(0.25)(decoder)\n",
    "\n",
    "# skip layer from conv5 (with added dropout)\n",
    "skip_4_drop = Dropout(0.25)(conv5)\n",
    "skip_4 = concatenate([decoder, skip_4_drop])\n",
    "\n",
    "# 14x14\n",
    "decoder = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_4)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Dropout(0.25)(decoder)\n",
    "\n",
    "# skip layer from conv4_ (with added dropout)\n",
    "skip_3_drop = Dropout(0.25)(conv4_)\n",
    "skip_3 = concatenate([decoder, skip_3_drop])\n",
    "\n",
    "# 28x28\n",
    "decoder = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_3)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Dropout(0.25)(decoder)\n",
    "\n",
    "# 56x56\n",
    "decoder = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "decoder = Dropout(0.25)(decoder)\n",
    "\n",
    "# 112x112\n",
    "decoder = Conv2DTranspose(64, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "\n",
    "# 112x112\n",
    "decoder = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(decoder)\n",
    "decoder = Activation('relu')(decoder)\n",
    "\n",
    "# 224x224\n",
    "# output layer, with 2 channels (a and b)\n",
    "output_layer = Conv2D(2, (1, 1), activation='tanh')(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure model to be optimized using Adam optimizer, specified learning rate and mean-squared error loss\n",
    "\n",
    "- This step loads the weights derived upon training our deep learning model for 100 epochs on the entire MIRFLICKR25k dataset (in batches of 3000 each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure model\n",
    "model = Model(net_input, output_layer)\n",
    "model.compile(Adam(lr=0.0002), loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# load weights\n",
    "model.load_weights('trained_on_all_is_for_100_es.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define all functions for getting predictions and constructing complete colorized RGB image based on LAB inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(model, image_l):\n",
    "    \"\"\"\n",
    "    Summary -\n",
    "    This function generates the predicted A and B channels of the colorized image\n",
    "\n",
    "    :param model: Trained model\n",
    "    :type model: tensorflow.python.keras.engine.training.Model\n",
    "    :param image_l: The L channel of the input grayscale image\n",
    "    :type image_l: numpy.ndarray\n",
    "    :return: Predicted A and B channels of the input grayscale image\n",
    "    :rtype: numpy.ndarray\n",
    "    \n",
    "    \"\"\"\n",
    "    # repeat the L value to match input shape\n",
    "    image_l_R = np.repeat(image_l[..., np.newaxis], 3, -1)\n",
    "    image_l_R = image_l_R.reshape((1, 224, 224, 3))\n",
    "    # normalize the input\n",
    "    image_l_R = (image_l_R.astype('float32') - 127.5) / 127.5\n",
    "    # make prediction\n",
    "    prediction = model.predict(image_l_R)\n",
    "    # normalize the output\n",
    "    pred = (prediction[0].astype('float32') * 127.5) + 127.5\n",
    "    return pred\n",
    "\n",
    "def get_LAB(image_l, image_ab):\n",
    "    \"\"\"\n",
    "    Summary - \n",
    "    This function generates the compiled RGB equivalent of the inputted L plus the predicted A and B channels\n",
    "\n",
    "    :param image_l: Grayscale image (1 layer, L)\n",
    "    :type image_l: numpy.ndarray\n",
    "    :param image_ab: Predicted image (2 layers, A and B)\n",
    "    :type image_ab: numpy.ndarray\n",
    "    :return: Compiled image (3 layers, RGB)\n",
    "    :rtype: numpy.ndarray\n",
    "    \n",
    "    \"\"\"\n",
    "    image_l = image_l.reshape((224, 224, 1))\n",
    "    image_lab = np.concatenate((image_l, image_ab), axis=2)\n",
    "    image_lab = image_lab.astype(\"uint8\")\n",
    "    image_rgb = cv.cvtColor(image_lab, cv.COLOR_LAB2RGB)\n",
    "    image_rgb = Image.fromarray(image_rgb)\n",
    "    return image_rgb\n",
    "\n",
    "def create_sample(model, image_gray):\n",
    "    \"\"\"\n",
    "    Summary - \n",
    "    This function creates the output we need from colorization\n",
    "\n",
    "    :param model: Trained model\n",
    "    :type model: tensorflow.python.keras.engine.training.Model\n",
    "    :param image_gray: Grayscale image (1 layer, L)\n",
    "    :type image_gray: numpy.ndarray\n",
    "    :return: Result image\n",
    "    :rtype: numpy.ndarray\n",
    "    \n",
    "    \"\"\"\n",
    "    # get the model's prediction\n",
    "    pred = get_pred(model, image_gray)\n",
    "    # combine input and output to LAB image\n",
    "    image = get_LAB(image_gray, pred)\n",
    "    # create new combined image, save it\n",
    "    new_image = Image.new('RGB', (224, 224))\n",
    "    new_image.paste(image, (0, 0))\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for image restoration, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(Selection,l,i):\n",
    "    \"\"\"\n",
    "    Summary -\n",
    "    Selects the image based on user choice and sends it to the deep learning model for processing\n",
    "    \n",
    "    \"\"\"\n",
    "    index = l.index(Selection)\n",
    "    img = i[index]\n",
    "    mod_img = resize(rgb2lab(img.copy())[:,:,0], (224,224), anti_aliasing=True)\n",
    "    sample = create_sample(model, mod_img)\n",
    "    sample.save('output/output.jpg')\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols = 2, figsize = (15,5))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(Selection)\n",
    "\n",
    "    axes[1].imshow(sample)\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('Auto-Colored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_restoration(img):\n",
    "    \"\"\"\n",
    "    Summary -\n",
    "    This function is created to carry out image restoration process in steps\n",
    "    \n",
    "    \"\"\"\n",
    "    #perform denoising of the image\n",
    "    denoised = cv.fastNlMeansDenoisingColored(img,None,7,10,6,21)\n",
    "    #canny edge detection\n",
    "    edges = cv.Canny(denoised,200,250)\n",
    "    #filter for image processing\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    #image dilation\n",
    "    dilation = cv.morphologyEx(edges, cv.MORPH_DILATE, kernel)\n",
    "    closing = cv.morphologyEx(dilation, cv.MORPH_CLOSE, kernel)\n",
    "    erode = cv.morphologyEx(closing,cv.MORPH_ERODE, kernel)\n",
    "    #fill in missing gaps \n",
    "    inpainted = cv.inpaint(denoised,erode,5,cv.INPAINT_TELEA)\n",
    "    #overlay denoised image over smoothed out image\n",
    "    alpha = 0.5\n",
    "    overlaid = inpainted.copy()\n",
    "    cv.addWeighted(denoised, alpha, overlaid, 1 - alpha,0, overlaid)\n",
    "    \n",
    "    #convert image to gray\n",
    "    img2gray = cv.cvtColor(denoised,cv.COLOR_BGR2GRAY)\n",
    "    #create mask and inverse mask based on threshold\n",
    "    ret, mask = cv.threshold(img2gray, 100, 255, cv.THRESH_BINARY_INV)\n",
    "    #combine via bit addition denoised image human and smoothed out background of inpainted image\n",
    "    bg1 = cv.bitwise_and(denoised,denoised,mask = mask)\n",
    "    mask_inv = cv.bitwise_not(mask)\n",
    "    bg2 = cv.bitwise_and(overlaid,overlaid,mask = mask_inv)\n",
    "    combined = cv.add(bg1,bg2)\n",
    "    \n",
    "    #store the various processed images\n",
    "    images = [img,denoised,inpainted,overlaid,combined]\n",
    "    labels = ['Original','Choice 1','Choice 2', 'Choice 3', 'Choice 4']\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images,labels):\n",
    "    \"\"\"\n",
    "    Summary - \n",
    "    This function displays the various processed images and labels associated with them\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(ncols = 5, figsize = (15,5))\n",
    "    axes[0].imshow(images[0])\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title(labels[0])\n",
    "\n",
    "    axes[1].imshow(images[1])\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title(labels[1])\n",
    "    \n",
    "    axes[2].imshow(images[2])\n",
    "    axes[2].axis('off')\n",
    "    axes[2].set_title(labels[2])\n",
    "    \n",
    "    axes[3].imshow(images[3])\n",
    "    axes[3].axis('off')\n",
    "    axes[3].set_title(labels[3])\n",
    "    \n",
    "    axes[4].imshow(images[4])\n",
    "    axes[4].axis('off')\n",
    "    axes[4].set_title(labels[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
      "      - Validating: \u001b[32mOK\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb86396ad108472fa2b2229317c87aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Enter the name of the picture you want to restore:'), HBox(children=(Text(value='<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92afcb7dd1fa4817b7e453f149398cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interact_manual, fixed, Output\n",
    "\n",
    "#create button for widget\n",
    "button = widgets.Button(description=\"Begin Restoration\")\n",
    "save = widgets.Button(description=\"Save Result\")\n",
    "#create text box for widget\n",
    "filename = widgets.Text(value='<filename>.jpg',placeholder='Type something',disabled=False)\n",
    "#create output area for widget\n",
    "output = widgets.Output()\n",
    "\n",
    "#layout setting piece for code\n",
    "vertical = widgets.VBox([widgets.Label(value=\"Enter the name of the picture you want to restore:\"),widgets.HBox([filename, button])])\n",
    "display(vertical)\n",
    "display(output)\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    \"\"\"\n",
    "    Summary -\n",
    "    This function handles the button clicked event\n",
    "    \n",
    "    \"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        #read the image\n",
    "        img = io.imread(\"images/\"+filename.value)\n",
    "        #gets 4 different kinds of processed images\n",
    "        images, labels = img_restoration(img)\n",
    "        #displays the various options\n",
    "        display_images(images,labels)\n",
    "        display(widgets.Label(value=\"Select the picture you want to color:\"))\n",
    "        #code that calls the model based on user selected image\n",
    "        interact(show_image, Selection = labels, l=fixed(labels), i=fixed(images), description = 'Choose image to color')\n",
    "        display(widgets.Label(value=\"Output saved to the output folder\"))\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
